\chapter{Design}
\label{design}

This chapter is centered around the components and concepts embodied in our network slicing architecture. We will first give an overview over our components and then present some concepts that will need to be implemented by our implementation.


\section{Components}
In order to present our components to achieve a network slicing architecture, we will group them by \acrshort{sdn} plane. We will begin by inspecting the data plane, followed by the control plane and conclude with the application plane. In figure \ref{fig:topology} you can already see the final result of our suggested topology, where the links that are used by the slice are marked green. Please note that other network architectures are possible as well and this is only an example topology. The focus is on the application and control plane components per network domain. All components described below are virtual network functions (\acrshort{vnf}s).

The presented components are partially sourced from Fuhrberg et al. \cite{SE4}. Our contribution is the addition of specialized components for the \gls{blacknetwork}, the redesign of logic on each of the components for our new distributed slice management approach and for the other concepts introduced in section \ref{design_concepts}.

\begin{landscape}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{images/chapter_5/topology.png}
        \caption[Slicing architecture components]{The components of our slicing architecture in an example topology. In this case we use 3 domains, one of which is a \gls{blacknetwork}. Per domain we use two switches. Apart from this, each edge domain has a host and a \acrshort{vpn} gateway. The green links are part of a slice from h1 to h2. The dotted links are bypass links for other traffic not using our slicing architecture because the \acrshort{vpn} gateways will not forward non-slice traffic.}
        \label{fig:topology}
    \end{figure}
\end{landscape}

\subsection{Data plane}
The job of the data plane is to transport traffic, so in our use case it will need to achieve the network resource isolation we require from our slices (protection goal \hyperref[P4]{P4}). To configure this isolation we need a way to assign traffic to \acrshort{qos} flows that span from one edge to another, traversing all required \gls{blacknetwork}s in the process. Furthermore we need to limit third party traffic so it may not impede our resource allocations. In order to perform easier routing decisions along our path, we wish to label traffic traversing a network slice according to the \acrshort{mpls} scheme. As a last data plane measure we require traffic to be encrypted and protected against modification while traversing the \gls{blacknetwork}s (protection goal \hyperref[P1]{P1} and \hyperref[P2]{P2}).

To deploy the \acrshort{qos} flows and secure them against third party traffic (protection goal \hyperref[P4]{P4}), we will need to make use of \acrshort{sdn} switches. The switches will forward the packets from source to destination. While the first switch will tag the packets with an \acrshort{mpls} label to facilitate easier routing and perform authentication of slice traffic along the entire path, the last switch will finally remove the \acrshort{mpls} label again. All switches will discard \acrshort{mpls} traffic from different ports than expected and drop packets with \acrshort{mpls} labels not expected to be found on a port. This way we can prevent third party traffic entering our slice, protecting the integrity of our slice.

The switches also have to limit traffic from other participants to guarantee the slice resources, which can be realised by creating a "default slice" for all non-slice traffic on the network. This can lead to underprovisioning of network resources however. The default slice should be shrunk when new slices are created to accommodate the new slices or have a relatively small size to not be able to disturb our slices. The slice resources can be guaranteed by setting up queues for all slices and the default slice to limit the overall possible outgoing traffic per port. For ingress traffic on ports, port ingress limits should be set up so that the switch may not be overloaded by the sheer amount of traffic from all sides.

For the encryption along the \gls{blacknetwork}s we will use \acrshort{vpn} gateways that encrypt traffic (protection goal \hyperref[P1]{P1}). The \acrshort{vpn} gateways will only ever see traffic from slices and should be shielded from all other traffic to limit the surface of attack. To connect to a remote end, \acrshort{vpn} gateways will use tunnels that need to be cryptographically secure to not disclose information and have protection against data manipulation to protect our data integrity (protection goal \ref{P2}). In order to limit the effect of information disclosure by the sheer existence of a slice through the \gls{blacknetwork}s, we will bundle multiple slices through one single tunnel. Packets from the tunnel will be tagged with another tunnel \acrshort{mpls} label by the first switch and the label will be removed again by the final tunnel switch due to the same reasons as with the original slice packets (integrity). A packet is assigned to a specific tunnel by examining the slice id in the \acrshort{mpls} label from the first slice switch.

%By tagging the packets with an \acrshort{mpls} label again and using the changing \acrshort{mpls} labels for routing, we effectively build a service function chain (SFC) to the destination host (via the two \acrshort{vpn} gateways), which partially anonymizes the recipient to the \gls{blacknetwork}s.

When a packet is traversing the tunnel, the unified guarantees for the individual slices should apply, meaning a sum of the individual \gls{bandwidth}s and the lowest values for \gls{latency}, \gls{jitter} and loss.

The last required components on the data plane are the hosts that wish to send or receive data via the slices. One of these hosts should contact the application plane to create a new slice before initiating communications with the remote host.

\subsection{Control plane}
The control plane should mainly support deploying our rules to our devices. Creating the flows is supported by using \Gls{openflow} through a standard \acrshort{sdn} controller. For our queues and ingress limits we will need a separate service realizing them on the switches though, as these features are not currently supported by OpenFlow. We suggest implementing them via a service running on the \acrshort{sdn} switches. For the \acrshort{vpn} gateways we require another service running on the specialized \acrshort{vpn} gateway hosts allowing us to register, resize and delete tunnels, as well as assigning slices to tunnels.

\subsection{Application plane}
The application plane will then care about defining the rules that should be deployed. In order to instruct the control plane, we will create one infrastructure coordinator per domain that will carry out all infrastructure tasks. This service that we will call a domain slice management function (\acrshort{dsmf}) will know about the entire topology and will have the routing information to other domains. It will set up domains and tunnels on the local infrastructure if requested and specify upfront whether resources can be guaranteed or not. This service thus follows an \acrshort{intserv} pattern to realize network slices.

Apart from this infrastructure service we will need to establish a domain coordination service, called an edge slice management function (\acrshort{esmf}). The \acrshort{esmf} will be used to coordinate the deployment of slices and tunnels across multiple domains. Apart from the coordination role it should also be the main endpoint for participants from the local domain to request slices through the network. By contacting other domains and the local infrastructure service the coordination service can evaluate whether a slice request from a participant is currently possible or not. For isolation purposes, only the coordination service of a domain (\acrshort{esmf}) will be allowed to contact the infrastructure service (\acrshort{dsmf}) of the same domain. The \acrshort{esmf}s will only talk between each other and with tenants of their own domain.

While \acrshort{esmf} and \acrshort{dsmf} allow the creation of slices and tunnels on their respective domain, versions of them also exist with reduced functionality for \gls{blacknetwork}s that do not wish to offer their tenants slice support. The \acrshort{esmf} will be replaced with a core tunnel management function (\acrshort{ctmf}) and the \acrshort{dsmf} will be replaced with a domain tunnel management function (\acrshort{dtmf}). A \acrshort{ctmf} will never initiate a new tunnel but rather receive tunnel requests from coordinating \acrshort{esmf}s. Also a \acrshort{ctmf} will never accept slice requests from tenants of their domain. They will thus only talk to an \acrshort{esmf} of another domain to coordinate new tunnels or with their own \acrshort{dtmf} to deploy tunnels to their infrastructure and to check for resource availability. A \acrshort{ctmf} may only talk to a \acrshort{dtmf} and an \acrshort{esmf} may only talk to a \acrshort{dsmf}, depending on the network role. \acrshort{ctmf} and \acrshort{dtmf} may only be deployed on \gls{blacknetwork}s that are exclusively \gls{blacknetwork}s, requiring their network domain to never be the beginning nor the end of a slice.

\paragraph{} After joining the different \acrshort{sdn} planes we thus reach our design topology (see figure \ref{fig:topology} for an example topology) that has the separation of different virtual network functions (\acrshort{vnf}s) in mind.


\section{Concepts}
\label{design_concepts}
After we presented our components we now want to mention our concepts behind certain design decisions, beginning with our concept of distributed trust and authority, our state synchronization and the strict separation of \acrshort{sdn} planes.

\subsection{Distributed trust and authority}
Distributed trust and authority is one of our core requirements (see \hyperref[R2]{requirement R2}). While wanting to lend resources to participants of our and other networks participating in our slicing architecture and guaranteeing those resources to them, we do not want others taking deployment or infrastructure decisions for our local domain. This is why we separated the coordination across all domains and only expose the coordinators to the other domains. This way another domain can ask us to allocate resources for a new slice but we may deny the creation at any point in time. Still other domains can request slices and tunnels this way without having direct access or knowledge about our infrastructure.

\subsection{State synchronization}
In order to be able to coordinate slices and tunnels across multiple domains we need to synchronize a partial global state across them. Without this synchronization slices could potentially be deployed only in part, violating our requirement of the basic availability of a slice (see requirement \hyperref[R1]{R1} and protection goal \hyperref[P3]{P3}) when traffic can not pass all domains along the slice. We thus require to synchronize a global state for each slice between the coordinators.

We will realize this by leveraging the two-phase commit protocol (\acrshort{2pcp}) \cite{2pcp}. This way tunnel and slices will first be reserved throughout the network and if all reservations succeeded they can then be deployed. A failure of the coordinating \acrshort{esmf} or a participant during the commit phase is no critical issue to us, as the host will not get a positive reply for the slice registration then. We do not guarantee the creation of slices to our tenants, but rather guarantee that a registered slice performs according to specification. Of course the issue could arise, that partially created slices persist on the network this way, but those dangling components could also be removed afterwards, when the system recovers from a failure.

\subsection{Strict separation of SDN planes}
In order to reach our goal to be relatively hardware independent (requirement \hyperref[R3]{R3}) we want to clearly structure our services according to our \acrshort{sdn} planes. Of course this may not always be possible because for example our switch queue and ingress limit service will be vendor specific due to no unified \acrshort{api} being available at this point to manage these characteristics. For all other services, the design has been separated according to the \acrshort{sdn} planes to be able to swap out different components of the \acrshort{sdn} environment with ease. For example to use a switch of a different vendor, only the switch queue and ingress limit service will need to be implemented for this vendor while the rest of the components can remain in use. This includes the integration of hardware switches or servers, enabling our topology to be executed outside of our validation scenarios.