\chapter{Conclusion}
\label{conclusion}

Isolation of tenants is one of the key requirements for building a network based on the \gls{zerotrust} approach \cite{zerotrust}, which has become more and more relevant due to the increase of devices in \acrshort{iot} \cite{iotincrease} and the subsequently increasing threats to network architectures \cite{iotthreats}. Network slicing can be used to isolate certain devices on a network which has gained additional popularity since the advent of 5G \cite{5G1, 5G2, 5G3}, of which network slicing is an integral part.

In this thesis we designed and implemented a solution to create isolated network slices across multiple domains and evaluated it for our protection goals of confidentiality, integrity, availability and resilience. This has been performed by conducting carefully designed experiments against multiple different scenarios that resemble real-world network architectures. Part of these scenarios is also one distributed scenario, where individual domains are separated on individual network nodes, integrating a hardware switch for some performance measurements that are closer to the real-world behaviour of switches.

Apart from meeting these requirements, our proposed solution also features distributed coordination of network slices allowing multiple domains to work together without necessarily trusting each other. This enables us to work together with other parties to bootstrap network slices over multiple networks. Furthermore our solution has been built with compatibility in mind by separating components, using common standards like \Gls{openflow} and providing specifications for our \acrshort{api}s. We also provide flexibility of network slices, allowing tenants to request multiple slices to multiple different target domains

Considering our initial example of a remote surgery robot, we can thus confidently state that the connection from the surgeon to the robot would provide significantly more resistance against attacks than before and as of now, the connection could not be broken by our attackers. For a patient, this can mean a better and longer life as opposed to the consequences of failure in a surgery.


\section{Limitations}
As with any solution, there are some limitations though. First, only two tenants may be part of a network slice, because a network slice is a communication channel in our solution. To include more tenants one can always build a mesh of communication channels though.

Closely related to this is that currently, network slices are always expected to go to another domain. Requesting a slice to the own local domain is not possible however. This functionality could be implemented in the future though.

A bigger limitation is currently, that we do not pass protection goals for availability and resilience if domain coordinators are attacked by \acrshort{http} floods or other \acrshort{dos} methods. This could be mitigated by protecting the coordinators against these attacks from their tenants, for example by deploying anti \acrshort{dos} solutions and firewalls. This has not been investigated however.


\section{Future work}
Many different things would come to mind when thinking about potential future work. One direction would be progressing the implementation further to production readiness. Requirements for this would be resilience against \acrshort{dos} on the coordinators, a proper authentication scheme and implementation of our \acrshort{vnf}s for vendor specific hardware. Furthermore our \acrshort{api}s would need to be protected for attacks on confidentiality and integrity, for example by deploying TLS to all services. Also more validation on \acrshort{api} endpoints could be performed in advance, so that requests can be rejected earlier before even asking other services to perform changes or running into errors.

Furthermore our implementation currently does not validate the successful creation and deletion of flows on the switches. This would need to be confirmed by a production-ready solution. While we were confident that \Gls{openflow} (\acrshort{udp}) messages would not get lost on their way from the controllers to the switches in our test scenarios, this may not hold true in reality.

The tunnel allocation logic could also be adapted to match the needs of the individual networks more closely. This could also incorporate sending a single packet on allocation to force the tunnel handshake so the initial packets of a tunnel get delivered faster.

The second direction would be progressing more on features. One feature could be accounting, where tenants would invest a currency to establish and keep a slice. This could either be used for billing these tenants or to allocate communication contingents to them apart from the already implemented parallel resource allocation limits. This is for example already common practice in mobile broadband.

Apart from this, another feature could be to set time limits on slices upon allocation, so that slices get removed automatically after a specified time or if they have not been used in a while. This could help limit a potential overflow in our slicing architecture after some time of operation.

Lastly, another interesting option to explore would be to actually factor in the \gls{latency} with our architecture. Currently, the architecture just provides the best \gls{latency} that it can offer by choosing and securing the shortest path if possible. In the future the \gls{latency} requirements could be factored in to choose an appropriate path through the network that can deliver the required \gls{latency} but might be under less load.